#import dataset
import kagglehub
# Download latest version
path = kagglehub.dataset_download("nelgiriyewithana/emotions")
print("Path to dataset files:", path)

import os
path = "/root/.cache/kagglehub/datasets/nelgiriyewithana/emotions/versions/1"
files_and_directories = os.listdir(path)
print("Files in the directory:")
for item in files_and_directories:
  print(item)

#load dataset
import pandas as pd
csv_file_name = 'text.csv'
csv_file_path = os.path.join(path, csv_file_name)
if os.path.exists(csv_file_path):
  df = pd.read_csv(csv_file_path)
texts = df['text'].tolist()
labels = df['label'].tolist()
print(f"\nFirst 5 rows of {csv_file_name}:")
display(df.head())

#split dataset
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(
    texts, labels, test_size=0.2, stratify=labels, random_state=42)

#tokenizer
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=50)
val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=50)

#check text length
all_lens = [len(tokenizer.tokenize(t)) for t in texts]
import numpy as np
print("average token length:", np.mean(all_lens))
print("95th percentile:", np.percentile(all_lens, 95))

#convert tensorflow for training
import tensorflow as tf
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    y_train
)).batch(32)
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    y_val
)).batch(32)

#load BERT pretrained
from transformers import TFBertForSequenceClassification
# dense layer
num_labels = len(set(labels))
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)

#compile
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)
history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=2
)

#evaluation model
import numpy as np
y_pred_logits = model.predict(val_dataset).logits
y_pred = np.argmax(y_pred_logits, axis=1)
y_true = []
for _, label in val_dataset.unbatch():
    y_true.append(label.numpy())

import matplotlib.pyplot as plt
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title("Training vs Validation Metrics")
plt.show()

model.evaluate(val_dataset)
model.save_pretrained("bert_emotion_model")
tokenizer.save_pretrained("bert_emotion_model")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print("Accuracy :", accuracy_score(y_true, y_pred))
print("Precision:", precision_score(y_true, y_pred, average='macro'))
print("Recall   :", recall_score(y_true, y_pred, average='macro'))
print("F1 Score :", f1_score(y_true, y_pred, average='macro'))

#prediction testing
text = "I'm very happy today" # type whatever you want for testing
inputs = tokenizer(text, return_tensors="tf", truncation=True, padding=True)
outputs = model(inputs)
pred_label = tf.argmax(outputs.logits, axis=1).numpy()[0]
print("Class Prediction:", le.inverse_transform([pred_label])[0])

 # Class index
  #      0: 'sad',
  #      1: 'happy',
  #      2: 'love',
  #      3: 'anger',
  #      4: 'fear',
  #      5: 'surprised'
